{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Enrichment and Further Modelling\n",
    "\n",
    "In this notebook, we will try adding more characters to the dataset by several ways:\n",
    "- Changing the threshold filtering rule\n",
    "- Splitting sentences that are too long into several sentences\n",
    "\n",
    "After reprocessing the dataset, we can proceed with the more advanced methodologies and also repeat the baselines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_long_sentence(text, max_tokens=128, tokenizer=None):\n",
    "    \"\"\"Split a long sentence into chunks with token length ≤ max_tokens.\"\"\"\n",
    "    if tokenizer is None:\n",
    "        return [text]\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for sent in sentences:\n",
    "        temp = f\"{current_chunk} {sent}\".strip() if current_chunk else sent\n",
    "        if len(tokenizer.encode(temp, add_special_tokens=True)) <= max_tokens:\n",
    "            current_chunk = temp\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # Fallback: recursively break anything still too long\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        while len(tokenizer.encode(chunk, add_special_tokens=True)) > max_tokens:\n",
    "            words = chunk.split()\n",
    "            mid = len(words) // 2\n",
    "            part1 = \" \".join(words[:mid])\n",
    "            part2 = \" \".join(words[mid:])\n",
    "            chunk = part2\n",
    "            final_chunks.append(part1)\n",
    "        final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reynold shouts at his mom once he realizes she threw his toy away. She apologizes but he’s furious. He runs into his room and slams the door.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Reynold shouts at his mom once he realizes she threw his toy away. She apologizes but he’s furious. He runs into his room and slams the door.\"\n",
    "chunks = split_long_sentence(text, max_tokens=128, tokenizer=tokenizer)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the example use case of the token count checking for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data and test the function\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"..//data//new_moral_only_data.json\", \"r\") as f:\n",
    "    moral_only_data = json.load(f)\n",
    "    moral_only_data = moral_only_data[\"moral_dialogue\"]\n",
    "\n",
    "with open(\"..//data//new_non_moral_data.json\", \"r\") as f:\n",
    "    non_moral_only_data = json.load(f)\n",
    "    non_moral_only_data = non_moral_only_data[\"moral_dialogue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['moral_dialogue', 'moral_dialogue_masked', 'ground_truth'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moral_only_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all_long_sentence_in_data(data: dict[dict[list]], max_tokens=128, tokenizer=None):\n",
    "    \"\"\" Process the data to split long sentences into smaller chunks.\n",
    "    \n",
    "    Args: \n",
    "    data (dictionary): dictionary containing dialogue data.\n",
    "    max_tokens (int): Maximum number of tokens per chunk.\n",
    "    tokenizer (transformers.PreTrainedTokenizer): Tokenizer to use for encoding text.\n",
    "    \"\"\"\n",
    "    for movie, characters in tqdm(data.items()):\n",
    "        for character, sentences in characters.items():\n",
    "            processed_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if tokenizer is not None:\n",
    "                    token_len = len(tokenizer.encode(sentence, add_special_tokens=True))\n",
    "                    if token_len > max_tokens:\n",
    "                        processed_sentences.extend(split_long_sentence(sentence, max_tokens, tokenizer))\n",
    "                    else:\n",
    "                        processed_sentences.append(sentence)\n",
    "                else:\n",
    "                    processed_sentences.append(sentence)\n",
    "            data[movie][character] = processed_sentences\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1629/1629 [00:03<00:00, 472.90it/s]\n",
      "100%|██████████| 1629/1629 [00:06<00:00, 266.03it/s]\n"
     ]
    }
   ],
   "source": [
    "moral_only_data_processed = split_all_long_sentence_in_data(moral_only_data, max_tokens=128, tokenizer=tokenizer)\n",
    "non_moral_only_data_processed = split_all_long_sentence_in_data(non_moral_only_data, max_tokens=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences longer than 128 tokens: 0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for movie, characters in non_moral_only_data_processed.items():\n",
    "    for character, sentences in characters.items():\n",
    "        for sentence in sentences:\n",
    "            if len(tokenizer.encode(sentence, add_special_tokens=True)) > 128:\n",
    "                count += 1\n",
    "\n",
    "print(f\"Number of sentences longer than 128 tokens: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love you.',\n",
       " \"What's all the trouble, Cinderella? What are you crying about, huh?\",\n",
       " 'Please, believe me. This is probably a stag film. Simulated rape. Hard to stomach, and it might seem real, but there are ways of making it look realistic. fake blood and special effects.',\n",
       " 'You. you need to go to the police.',\n",
       " \"Few days ago, I was contacted by a couple living in Philadelphia, a doctor and his wife. What happened was they picked up a young girl hitchhiking off 81, which heads into Philadelphia, started up a conversation with this girl, she looked homeless, seemed about eighteen maybe. They convinced her to let them buy her a meal in the city. Nice kid, mature, did n't have much to say, but they got a sense she's a runaway, so all through dinner the doctor's working on her, trying to convince her that at the very least she should pick up a telephone.\",\n",
       " 'Not surprisingly, she ate her food, excused herself.',\n",
       " \"This doctor and wife, they're nice people, but they do n't want to get too involved. They're not trying to have the parents come looking for the girl either. You and I both know sometimes, not often, but sometimes there's real reasons why a kid'll run. Molestation, whatever. Besides that, the girl's probably eighteen, so she's legal.\",\n",
       " \"I'm sorry, I know this is n't easy. Is there a more convenient time?\",\n",
       " \"If there's anything you feel uncomfortable talking about, tell me, but I have to ask. Your husband. he committed suicide?\",\n",
       " 'You have to forgive me, but in these circumstances. with your daughter.',\n",
       " \"I've got a lead I have to follow through. To be honest, I do n't think I'm going to get very far. I miss you. I love you.\",\n",
       " \"It's not really my place, but it's not easy for me to set aside the private detective part of me either. See, I know a little about missing persons. When kids run, they almost always leave a note. It's guilt. They want to say goodbye.\",\n",
       " 'Do you think the police did a good job?',\n",
       " \"It is possible. and I know this is n't something you want to hear. Your daughter may have tried to hide a note where she thought you would eventually find it, but where she knew your husband would never find it. She might have wanted to tell you something.\",\n",
       " \"If the police focused their search in her room, her belongings, well that'd be only natural, but they may have been looking in the wrong place.\",\n",
       " 'Do you ever consider. do you realize that Mary may never come back?',\n",
       " 'I miss you too. I love you very much. Give Cinderella a kiss for me and tell her I love her, alright?',\n",
       " \"Well, here's the deal, Max. This thing I'm on right now has something to do with underground pornography. Stuff that's sold under the counter, illegally.\",\n",
       " \"Well, whatever there is, whoever's dealing, however it's done, I want to know. I want a good look, so if you've got that kind of connection, great. If not, speak now.\",\n",
       " 'Child pornography.',\n",
       " 'Snuff films.',\n",
       " \"For a human life. murder on film, no statute of limitations. Who knows? It sure could have. I'd like you to overnight me a copy of those checks, then put them in a safe deposit box.\",\n",
       " 'You murdered that girl, Eddie. Six years ago.',\n",
       " \"You killed that girl and you put it on film. You and your pals, you're fucked. You fucked up real good.\",\n",
       " 'Five thousand now, five thousand on delivery. Two women, one white and one black, as long as they have large breasts. Hard bondage, or course. Other than that, trusting your artistic interpretation, I have only two stipulations.',\n",
       " \"It's not my money. The woman I got it from is never going to give it a second thought. Let's not make a big deal out of this, okay?\",\n",
       " \"You were the middleman, am I right? Old man Christian was n't about to go shopping for a snuff film himself.\",\n",
       " \"So, he sent you, gave you the money, his errand - boy. And if you refused, it was n't like you could tell anyone your pervert boss just asked you to get him a snuff film. That's the beauty of lawyer/client privilege.\",\n",
       " \"Must have paid you a lot, for you to risk everything. Would've had to have cut yourself a real nice piece of money.\",\n",
       " \"That's why you got scared when Mrs. Christian hired me. You knew about the film, figured it had to be in that safe. How'd you find me?\",\n",
       " \"Except, you're willing commit murder with them.\",\n",
       " \"You found these smut dealers and asked to buy a snuff film, right? Wanted them to find you one. Well, they did n't find you one, Longdale, they went out and made you one.\",\n",
       " 'Mary Anne Mathews was alive till you paid money to have her murdered.',\n",
       " 'Just tell me. Tell me some more of the secrets you and Christian shared. What kind of degenerate pervert was he really? What the fuck did he want with a snuff film?',\n",
       " 'Why did he buy a film of some poor, lost girl getting butchered?',\n",
       " \"I know why you did it, Dino, Eddie. but, why'd the lawyer do it? Must have been a helluva lot of money, right? One fuckload of money.\",\n",
       " 'Look at him. You think he played it square? How much did he give you, how much did he keep for himself?',\n",
       " \"No, no, no. please, do n't kill me. please!\",\n",
       " \"You might make it to your gun, but not before I shoot Machine. And if I have to shoot him because of you, and I do n't kill him, right after he kills me, he's gon na kill you.\",\n",
       " \"I ca n't tell you, Amy. You know I ca n't. You have to trust me.\",\n",
       " \"I was in hell. If I called you. if I heard your voice. it would have been so easy for me to quit. I could n't do that.\",\n",
       " 'Forgive me.',\n",
       " 'I will never get tired of hurting you, Eddie, so you might want to change your attitude.',\n",
       " \"Okay, we'll come back to that. So, six years ago a guy contacts you, through the classifieds, over the phone, however he does it. It's Longdale, looking for a snuff film. And you, entrepreneur that you are, tell him you can hook him up.\",\n",
       " 'Told him you could get him a snuff film.',\n",
       " \"That's all? Thirty each. That's all it took for you to murder her?\",\n",
       " \"That night. you did n't have to be in the room, but you were.\",\n",
       " 'I can kill you. I can leave you out here, just like you left her.',\n",
       " \"I have to tell you something. It wo n't be easy for you to hear. It's about your daughter. Mary Anne.\",\n",
       " 'Someone. some men, they took your daughter and they drugged her, and they took her to a motel room. they did terrible things to her.',\n",
       " 'They brought her into the room. one man, he put a knife to her throat and he raped her.',\n",
       " 'He raped her and. and. and he murdered her. he cut her up with knifes.',\n",
       " 'They killed her, and they took her out in the forest somewhere and they buried her.',\n",
       " \"They murdered her, Mrs. Mathews, I'm sorry. It happened a month after she ran away. She's been dead all this time.\",\n",
       " \"I have to tell you. I have to tell you what happened. I have to tell you everything, but we ca n't tell anyone else. No one else can ever know.\",\n",
       " \"You're the only one who can save me.\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moral_only_data_processed['8MM_1999'][\"WELLES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try re-filter the new_dialogue json so that it only contains sentences that are less than 128 tokens long.\n",
    "\n",
    "with open(\"..//data//new_dialogue.json\", \"r\") as f:\n",
    "    new_dialogue = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_dialogue = split_all_long_sentence_in_data(new_dialogue, max_tokens=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in new_new_dialogue: 1959645\n",
      "Total number of sentences in new_dialogue: 1955706\n",
      "We gained 3939 sentences by splitting long sentences.\n"
     ]
    }
   ],
   "source": [
    "count1 = 0\n",
    "for movie, characters in new_new_dialogue.items():\n",
    "    for character, sentences in characters.items():\n",
    "        count1 += len(sentences)\n",
    "\n",
    "print(f\"Total number of sentences in new_new_dialogue: {count1}\")\n",
    "\n",
    "count2 = 0\n",
    "for movie, characters in new_dialogue.items():\n",
    "    for character, sentences in characters.items():\n",
    "        count2 += len(sentences)\n",
    "\n",
    "print(f\"Total number of sentences in new_dialogue: {count2}\")\n",
    "\n",
    "print(f\"We gained {count1 - count2} sentences by splitting long sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "\n",
    "We will try scraping more movies from the internet to enrich our data so that the result is more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
