{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline using FastText Embeddings\n",
    "\n",
    "This acts as a baseline to be compared with the models using SBERT. \n",
    "\n",
    "Methodologies:\n",
    "- For each sentence, we take the average of all word embeddings in the sentence to represent the sentence embedding\n",
    "- We then do dimensionality reduction to get lower dimensional vectors since a too high dimension will cause overfitting\n",
    "- For each character, we average the latent sentence embeddings to give us a character representation\n",
    "- We pass this character embedding to predictor to get moral ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//data//structured_data_full.json\", \"r\") as f:\n",
    "    structured_data_full = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Movies: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 91.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load FastText vectors manually (limit can help you load faster)\n",
    "def load_fasttext_vecs(path, limit=200000):\n",
    "    vectors = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # skip header line\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            parts = line.rstrip().split(' ')\n",
    "            word = parts[0]\n",
    "            vec = np.array(parts[1:], dtype=float)\n",
    "            vectors[word] = vec\n",
    "    return vectors\n",
    "\n",
    "fasttext_vectors = load_fasttext_vecs(\"..//model//cc.en.300.vec\", limit=200000)\n",
    "\n",
    "# Helper\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "sentence_types = [\"moral\", \"non_moral\", \"action\", \"adj\"]\n",
    "fasttext_embedding_dictionary = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for movie, char_dict in tqdm(structured_data_full.items(), desc=\"Movies\"):\n",
    "    for character, type_dict in tqdm(char_dict.items(), desc=f\"{movie} Characters\", leave=False):\n",
    "        for type_ in sentence_types:\n",
    "            sentences = type_dict.get(type_, [])\n",
    "\n",
    "            if not sentences:\n",
    "                fasttext_embedding_dictionary[movie][character][type_] = []\n",
    "                continue\n",
    "\n",
    "            sentence_embeddings = []\n",
    "            for sentence in sentences:\n",
    "                tokens = tokenize(sentence)\n",
    "                vectors = [fasttext_vectors[token] for token in tokens if token in fasttext_vectors]\n",
    "                if vectors:\n",
    "                    vec = np.mean(vectors, axis=0)\n",
    "                    vec /= np.linalg.norm(vec) if np.linalg.norm(vec) > 0 else 1  # optional normalization\n",
    "                else:\n",
    "                    vec = np.zeros(300)\n",
    "                sentence_embeddings.append(vec)\n",
    "\n",
    "            fasttext_embedding_dictionary[movie][character][type_] = sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_index_to_name = {\n",
    "    14: \"cunningâ€“honorable\",\n",
    "    22: \"ferociousâ€“pacifist\",\n",
    "    25: \"forgivingâ€“vengeful\",\n",
    "    28: \"loyalâ€“traitorous\",\n",
    "    31: \"rudeâ€“respectful\",\n",
    "    38: \"arrogantâ€“humble\",\n",
    "    39: \"heroicâ€“villainous\",\n",
    "    42: \"mischievousâ€“well-behaved\",\n",
    "    62: \"confidentâ€“insecure\",\n",
    "    64: \"debasedâ€“purity\",  # we added this trait\n",
    "    79: \"selfishâ€“altruistic\",\n",
    "    81: \"angelicâ€“demonic\",\n",
    "    84: \"cruelâ€“kind\",\n",
    "    85: \"directâ€“roundabout\",\n",
    "    101: \"biasedâ€“impartial\",\n",
    "    121: \"sarcasticâ€“genuine\",\n",
    "    134: \"obedientâ€“rebellious\",   # we added this trait\n",
    "    154: \"judgementalâ€“accepting\",\n",
    "    195: \"complimentaryâ€“insulting\",\n",
    "    222: \"wholesomeâ€“salacious\",\n",
    "    # 224: \"zanyâ€“regular\",\n",
    "    227: \"racistâ€“egalitarian\",\n",
    "    390: \"transparentâ€“machiavellian\",\n",
    "    396: \"innocentâ€“jaded\",\n",
    "    # 425: \"flawedâ€“perfect\",\n",
    "    434: \"resentfulâ€“euphoric\",\n",
    "    441: \"buffoonâ€“charmer\",   # buffoon is a synonym for clown\n",
    "    448: \"fakeâ€“real\",\n",
    "    450: \"cattyâ€“supportive\",\n",
    "    453: \"eagerâ€“reluctant\",\n",
    "    464: \"forwardâ€“repressed\",\n",
    "    485: \"maverickâ€“conformist\",\n",
    "    # 487: \"social chameleonâ€“strong identity\",   # social chameleon is a person who changes their behavior to fit in with different social groups\n",
    "    489: \"sincereâ€“irreverent\",\n",
    "    # 494: \"hopefulâ€“fearful\",\n",
    "    # 495: \"likes changeâ€“resists change\",\n",
    "    # 497: \"old-fashionedâ€“progressive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "latent_dim = 20\n",
    "reduction_method = \"pca\"   # or \"ae\"\n",
    "model_type = \"ridge\"       # or \"mlp\"\n",
    "\n",
    "# selected_indices = [trait_index_dict[t] for t in selected_traits]\n",
    "moral_trait_indices = [\n",
    "    14, 22, 25, 28, 31, 38, 39, 42, 62, 64, 79, 81, 84, 85, 101, 121, 134, 154,\n",
    "    195, 222, 227, 390, 396, 434, 441, 448, 450, 453,\n",
    "    464, 485, 489\n",
    "]\n",
    "\n",
    "\n",
    "sentence_types = [\"moral\", \"non_moral\", \"action\", \"adj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_moral_trait_prediction(\n",
    "    fasttext_embedding_dictionary,\n",
    "    structured_data_full,\n",
    "    trait_index_to_name,\n",
    "    moral_trait_indices,\n",
    "    sentence_types=[\"moral\", \"non_moral\", \"action\", \"adj\"],\n",
    "    reduction_method=\"pca\",\n",
    "    model_type=\"ridge\",\n",
    "    latent_dim=20,\n",
    "    ae_epochs=100\n",
    "):\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    print(\"ðŸ“¦ Collecting all sentence embeddings...\")\n",
    "    all_embeddings = []\n",
    "    for movie in fasttext_embedding_dictionary:\n",
    "        for char in fasttext_embedding_dictionary[movie]:\n",
    "            for t in sentence_types:\n",
    "                all_embeddings.extend(fasttext_embedding_dictionary[movie][char].get(t, []))\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "    print(f\"ðŸ”§ Training {reduction_method.upper()} reducer...\")\n",
    "    if reduction_method == \"pca\":\n",
    "        reducer = PCA(n_components=latent_dim)\n",
    "        reducer.fit(all_embeddings)\n",
    "        reduce_fn = lambda X: reducer.transform(X)\n",
    "    elif reduction_method == \"ae\":\n",
    "        class AE(nn.Module):\n",
    "            def __init__(self, input_dim=300, latent_dim=latent_dim):\n",
    "                super().__init__()\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(input_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, latent_dim)\n",
    "                )\n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, input_dim)\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                z = self.encoder(x)\n",
    "                return z, self.decoder(z)\n",
    "\n",
    "        # ae = AE().cuda()\n",
    "        ae = AE()\n",
    "        optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        # X_tensor = torch.tensor(all_embeddings, dtype=torch.float32).cuda()\n",
    "        X_tensor = torch.tensor(all_embeddings, dtype=torch.float32)\n",
    "\n",
    "        for epoch in range(ae_epochs):\n",
    "            ae.train()\n",
    "            z, recon = ae(X_tensor)\n",
    "            loss = loss_fn(recon, X_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ae.eval()\n",
    "        # reduce_fn = lambda X: ae.encoder(torch.tensor(X, dtype=torch.float32).cuda()).detach().cpu().numpy()\n",
    "        reduce_fn = lambda X: ae.encoder(torch.tensor(X, dtype=torch.float32)).detach().cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Reduction method must be 'pca' or 'ae'.\")\n",
    "\n",
    "    print(\"Building feature matrix...\")\n",
    "    X, y = [], []\n",
    "    for movie, chars in fasttext_embedding_dictionary.items():\n",
    "        for char, data in chars.items():\n",
    "            all_sentences = []\n",
    "            for t in sentence_types:\n",
    "                all_sentences.extend(data.get(t, []))\n",
    "\n",
    "            if not all_sentences:\n",
    "                continue\n",
    "\n",
    "            reduced = reduce_fn(np.vstack(all_sentences))\n",
    "            avg_vector = reduced.mean(axis=0)\n",
    "            X.append(avg_vector)\n",
    "\n",
    "            if \"rating\" in structured_data_full[movie][char] and len(structured_data_full[movie][char][\"rating\"]) >= max(moral_trait_indices) + 1:\n",
    "                y.append([structured_data_full[movie][char][\"rating\"][i] for i in moral_trait_indices])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(f\"Final shape: X = {X.shape}, y = {y.shape}\")\n",
    "\n",
    "    print(f\"Training {model_type.upper()} model...\")\n",
    "    if model_type == \"ridge\":\n",
    "        model = Ridge()\n",
    "    elif model_type == \"mlp\":\n",
    "        mlp_model = MLPRegressor(\n",
    "            hidden_layer_sizes=(32, ),\n",
    "            max_iter=2000,\n",
    "            early_stopping=True,\n",
    "            learning_rate='adaptive',\n",
    "            random_state=42,\n",
    "            alpha=0.01\n",
    "        )\n",
    "\n",
    "        model = make_pipeline(StandardScaler(), mlp_model)  # scale features\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type.\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    r2 = r2_score(y, y_pred, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred, multioutput='raw_values'))\n",
    "\n",
    "    print(\"\\n Moral Trait Prediction Results:\")\n",
    "    results = {}\n",
    "    for i, trait_idx in enumerate(moral_trait_indices):\n",
    "        trait_name = trait_index_to_name.get(trait_idx, f\"Trait {trait_idx}\")\n",
    "        print(f\"{trait_name:<35} | RÂ² = {r2[i]:.3f} | RMSE = {rmse[i]:.3f}\")\n",
    "        results[trait_name] = {\"R2\": r2[i], \"RMSE\": rmse[i]}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_all_models_and_save(\n",
    "    fasttext_embedding_dictionary,\n",
    "    structured_data_full,\n",
    "    trait_index_to_name,\n",
    "    moral_trait_indices,\n",
    "    filename=\"..//results//baseline_2_moral_trait_evaluation.xlsx\"\n",
    "):\n",
    "    results_dict = {}\n",
    "\n",
    "    for reduction_method in [\"pca\", \"ae\"]:\n",
    "        for model_type in [\"ridge\", \"mlp\"]:\n",
    "            key = f\"{reduction_method.upper()} + {model_type.upper()}\"\n",
    "            print(f\"\\nRunning: {key}\")\n",
    "            result = run_moral_trait_prediction(\n",
    "                fasttext_embedding_dictionary,\n",
    "                structured_data_full,\n",
    "                trait_index_to_name,\n",
    "                moral_trait_indices,\n",
    "                reduction_method=reduction_method,\n",
    "                model_type=model_type\n",
    "            )\n",
    "            for trait_name in result:\n",
    "                if trait_name not in results_dict:\n",
    "                    results_dict[trait_name] = {}\n",
    "                results_dict[trait_name][f\"{key} RÂ²\"] = result[trait_name][\"R2\"]\n",
    "                results_dict[trait_name][f\"{key} RMSE\"] = result[trait_name][\"RMSE\"]\n",
    "\n",
    "    # Convert to DataFrame and save to Excel\n",
    "    df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "    df.to_excel(filename)\n",
    "    print(f\"\\n Results saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: PCA + RIDGE\n",
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training PCA reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 31)\n",
      "Training RIDGE model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = 0.022 | RMSE = 21.206\n",
      "ferociousâ€“pacifist                  | RÂ² = 0.036 | RMSE = 20.919\n",
      "forgivingâ€“vengeful                  | RÂ² = 0.050 | RMSE = 16.945\n",
      "loyalâ€“traitorous                    | RÂ² = 0.010 | RMSE = 18.889\n",
      "rudeâ€“respectful                     | RÂ² = 0.042 | RMSE = 15.396\n",
      "arrogantâ€“humble                     | RÂ² = 0.029 | RMSE = 21.854\n",
      "heroicâ€“villainous                   | RÂ² = 0.034 | RMSE = 17.644\n",
      "mischievousâ€“well-behaved            | RÂ² = 0.053 | RMSE = 14.039\n",
      "confidentâ€“insecure                  | RÂ² = 0.056 | RMSE = 17.969\n",
      "debasedâ€“purity                      | RÂ² = 0.031 | RMSE = 16.191\n",
      "selfishâ€“altruistic                  | RÂ² = 0.012 | RMSE = 13.006\n",
      "angelicâ€“demonic                     | RÂ² = 0.117 | RMSE = 18.286\n",
      "cruelâ€“kind                          | RÂ² = 0.065 | RMSE = 15.148\n",
      "directâ€“roundabout                   | RÂ² = 0.042 | RMSE = 17.448\n",
      "biasedâ€“impartial                    | RÂ² = 0.041 | RMSE = 17.930\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.024 | RMSE = 19.015\n",
      "obedientâ€“rebellious                 | RÂ² = 0.044 | RMSE = 16.513\n",
      "judgementalâ€“accepting               | RÂ² = 0.040 | RMSE = 17.282\n",
      "complimentaryâ€“insulting             | RÂ² = 0.050 | RMSE = 14.961\n",
      "wholesomeâ€“salacious                 | RÂ² = 0.021 | RMSE = 18.427\n",
      "racistâ€“egalitarian                  | RÂ² = 0.028 | RMSE = 15.733\n",
      "transparentâ€“machiavellian           | RÂ² = 0.090 | RMSE = 18.985\n",
      "innocentâ€“jaded                      | RÂ² = 0.051 | RMSE = 17.322\n",
      "resentfulâ€“euphoric                  | RÂ² = 0.027 | RMSE = 19.502\n",
      "buffoonâ€“charmer                     | RÂ² = 0.035 | RMSE = 14.031\n",
      "fakeâ€“real                           | RÂ² = 0.029 | RMSE = 20.840\n",
      "cattyâ€“supportive                    | RÂ² = 0.016 | RMSE = 19.794\n",
      "eagerâ€“reluctant                     | RÂ² = 0.021 | RMSE = 18.434\n",
      "forwardâ€“repressed                   | RÂ² = 0.107 | RMSE = 19.744\n",
      "maverickâ€“conformist                 | RÂ² = 0.012 | RMSE = 19.696\n",
      "sincereâ€“irreverent                  | RÂ² = 0.030 | RMSE = 18.380\n",
      "\n",
      "Running: PCA + MLP\n",
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training PCA reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 31)\n",
      "Training MLP model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = 0.135 | RMSE = 19.938\n",
      "ferociousâ€“pacifist                  | RÂ² = 0.203 | RMSE = 19.020\n",
      "forgivingâ€“vengeful                  | RÂ² = 0.116 | RMSE = 16.348\n",
      "loyalâ€“traitorous                    | RÂ² = 0.010 | RMSE = 18.896\n",
      "rudeâ€“respectful                     | RÂ² = 0.188 | RMSE = 14.180\n",
      "arrogantâ€“humble                     | RÂ² = 0.197 | RMSE = 19.884\n",
      "heroicâ€“villainous                   | RÂ² = 0.258 | RMSE = 15.465\n",
      "mischievousâ€“well-behaved            | RÂ² = -0.146 | RMSE = 15.445\n",
      "confidentâ€“insecure                  | RÂ² = 0.180 | RMSE = 16.748\n",
      "debasedâ€“purity                      | RÂ² = 0.064 | RMSE = 15.918\n",
      "selfishâ€“altruistic                  | RÂ² = -0.394 | RMSE = 15.453\n",
      "angelicâ€“demonic                     | RÂ² = 0.153 | RMSE = 17.909\n",
      "cruelâ€“kind                          | RÂ² = 0.039 | RMSE = 15.351\n",
      "directâ€“roundabout                   | RÂ² = -0.023 | RMSE = 18.027\n",
      "biasedâ€“impartial                    | RÂ² = -0.109 | RMSE = 19.283\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.213 | RMSE = 17.075\n",
      "obedientâ€“rebellious                 | RÂ² = 0.169 | RMSE = 15.396\n",
      "judgementalâ€“accepting               | RÂ² = 0.120 | RMSE = 16.545\n",
      "complimentaryâ€“insulting             | RÂ² = -0.011 | RMSE = 15.433\n",
      "wholesomeâ€“salacious                 | RÂ² = 0.020 | RMSE = 18.435\n",
      "racistâ€“egalitarian                  | RÂ² = -0.049 | RMSE = 16.346\n",
      "transparentâ€“machiavellian           | RÂ² = 0.227 | RMSE = 17.504\n",
      "innocentâ€“jaded                      | RÂ² = 0.083 | RMSE = 17.030\n",
      "resentfulâ€“euphoric                  | RÂ² = -0.029 | RMSE = 20.056\n",
      "buffoonâ€“charmer                     | RÂ² = -0.227 | RMSE = 15.824\n",
      "fakeâ€“real                           | RÂ² = 0.044 | RMSE = 20.680\n",
      "cattyâ€“supportive                    | RÂ² = 0.071 | RMSE = 19.241\n",
      "eagerâ€“reluctant                     | RÂ² = 0.061 | RMSE = 18.055\n",
      "forwardâ€“repressed                   | RÂ² = 0.189 | RMSE = 18.820\n",
      "maverickâ€“conformist                 | RÂ² = -0.089 | RMSE = 20.676\n",
      "sincereâ€“irreverent                  | RÂ² = -0.005 | RMSE = 18.705\n",
      "\n",
      "Running: AE + RIDGE\n",
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training AE reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 31)\n",
      "Training RIDGE model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = 0.005 | RMSE = 21.382\n",
      "ferociousâ€“pacifist                  | RÂ² = 0.019 | RMSE = 21.106\n",
      "forgivingâ€“vengeful                  | RÂ² = 0.028 | RMSE = 17.143\n",
      "loyalâ€“traitorous                    | RÂ² = 0.004 | RMSE = 18.946\n",
      "rudeâ€“respectful                     | RÂ² = 0.023 | RMSE = 15.550\n",
      "arrogantâ€“humble                     | RÂ² = 0.011 | RMSE = 22.056\n",
      "heroicâ€“villainous                   | RÂ² = 0.013 | RMSE = 17.831\n",
      "mischievousâ€“well-behaved            | RÂ² = 0.029 | RMSE = 14.211\n",
      "confidentâ€“insecure                  | RÂ² = 0.031 | RMSE = 18.205\n",
      "debasedâ€“purity                      | RÂ² = 0.016 | RMSE = 16.319\n",
      "selfishâ€“altruistic                  | RÂ² = 0.005 | RMSE = 13.055\n",
      "angelicâ€“demonic                     | RÂ² = 0.068 | RMSE = 18.793\n",
      "cruelâ€“kind                          | RÂ² = 0.034 | RMSE = 15.396\n",
      "directâ€“roundabout                   | RÂ² = 0.017 | RMSE = 17.674\n",
      "biasedâ€“impartial                    | RÂ² = 0.026 | RMSE = 18.072\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.008 | RMSE = 19.175\n",
      "obedientâ€“rebellious                 | RÂ² = 0.024 | RMSE = 16.684\n",
      "judgementalâ€“accepting               | RÂ² = 0.022 | RMSE = 17.445\n",
      "complimentaryâ€“insulting             | RÂ² = 0.028 | RMSE = 15.128\n",
      "wholesomeâ€“salacious                 | RÂ² = 0.008 | RMSE = 18.545\n",
      "racistâ€“egalitarian                  | RÂ² = 0.013 | RMSE = 15.849\n",
      "transparentâ€“machiavellian           | RÂ² = 0.048 | RMSE = 19.424\n",
      "innocentâ€“jaded                      | RÂ² = 0.029 | RMSE = 17.524\n",
      "resentfulâ€“euphoric                  | RÂ² = 0.011 | RMSE = 19.668\n",
      "buffoonâ€“charmer                     | RÂ² = 0.013 | RMSE = 14.195\n",
      "fakeâ€“real                           | RÂ² = 0.015 | RMSE = 20.989\n",
      "cattyâ€“supportive                    | RÂ² = 0.009 | RMSE = 19.873\n",
      "eagerâ€“reluctant                     | RÂ² = 0.007 | RMSE = 18.571\n",
      "forwardâ€“repressed                   | RÂ² = 0.069 | RMSE = 20.157\n",
      "maverickâ€“conformist                 | RÂ² = 0.002 | RMSE = 19.796\n",
      "sincereâ€“irreverent                  | RÂ² = 0.011 | RMSE = 18.555\n",
      "\n",
      "Running: AE + MLP\n",
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training AE reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 31)\n",
      "Training MLP model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = -0.089 | RMSE = 22.375\n",
      "ferociousâ€“pacifist                  | RÂ² = 0.040 | RMSE = 20.882\n",
      "forgivingâ€“vengeful                  | RÂ² = -0.061 | RMSE = 17.913\n",
      "loyalâ€“traitorous                    | RÂ² = -0.131 | RMSE = 20.196\n",
      "rudeâ€“respectful                     | RÂ² = 0.006 | RMSE = 15.686\n",
      "arrogantâ€“humble                     | RÂ² = 0.080 | RMSE = 21.281\n",
      "heroicâ€“villainous                   | RÂ² = 0.111 | RMSE = 16.922\n",
      "mischievousâ€“well-behaved            | RÂ² = -0.308 | RMSE = 16.500\n",
      "confidentâ€“insecure                  | RÂ² = 0.021 | RMSE = 18.298\n",
      "debasedâ€“purity                      | RÂ² = -0.154 | RMSE = 17.672\n",
      "selfishâ€“altruistic                  | RÂ² = -0.730 | RMSE = 17.217\n",
      "angelicâ€“demonic                     | RÂ² = 0.044 | RMSE = 19.028\n",
      "cruelâ€“kind                          | RÂ² = -0.154 | RMSE = 16.828\n",
      "directâ€“roundabout                   | RÂ² = -0.364 | RMSE = 20.820\n",
      "biasedâ€“impartial                    | RÂ² = -0.298 | RMSE = 20.860\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.075 | RMSE = 18.514\n",
      "obedientâ€“rebellious                 | RÂ² = 0.057 | RMSE = 16.403\n",
      "judgementalâ€“accepting               | RÂ² = -0.082 | RMSE = 18.354\n",
      "complimentaryâ€“insulting             | RÂ² = -0.106 | RMSE = 16.137\n",
      "wholesomeâ€“salacious                 | RÂ² = -0.179 | RMSE = 20.222\n",
      "racistâ€“egalitarian                  | RÂ² = -0.270 | RMSE = 17.985\n",
      "transparentâ€“machiavellian           | RÂ² = 0.001 | RMSE = 19.889\n",
      "innocentâ€“jaded                      | RÂ² = -0.099 | RMSE = 18.638\n",
      "resentfulâ€“euphoric                  | RÂ² = -0.176 | RMSE = 21.442\n",
      "buffoonâ€“charmer                     | RÂ² = -0.391 | RMSE = 16.850\n",
      "fakeâ€“real                           | RÂ² = -0.135 | RMSE = 22.530\n",
      "cattyâ€“supportive                    | RÂ² = -0.014 | RMSE = 20.101\n",
      "eagerâ€“reluctant                     | RÂ² = -0.192 | RMSE = 20.346\n",
      "forwardâ€“repressed                   | RÂ² = 0.076 | RMSE = 20.083\n",
      "maverickâ€“conformist                 | RÂ² = -0.296 | RMSE = 22.558\n",
      "sincereâ€“irreverent                  | RÂ² = -0.158 | RMSE = 20.081\n",
      "\n",
      " Results saved to ..//results//baseline_2_moral_trait_evaluation.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_all_models_and_save(\n",
    "    fasttext_embedding_dictionary,\n",
    "    structured_data_full,\n",
    "    trait_index_to_name,\n",
    "    moral_trait_indices,\n",
    "    filename=\"..//results//baseline_2_moral_trait_evaluation.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training PCA reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 35)\n",
      "Training RIDGE model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = 0.022 | RMSE = 21.206\n",
      "ferociousâ€“pacifist                  | RÂ² = 0.036 | RMSE = 20.919\n",
      "forgivingâ€“vengeful                  | RÂ² = 0.050 | RMSE = 16.945\n",
      "loyalâ€“traitorous                    | RÂ² = 0.010 | RMSE = 18.889\n",
      "rudeâ€“respectful                     | RÂ² = 0.042 | RMSE = 15.396\n",
      "arrogantâ€“humble                     | RÂ² = 0.029 | RMSE = 21.854\n",
      "heroicâ€“villainous                   | RÂ² = 0.034 | RMSE = 17.644\n",
      "mischievousâ€“well-behaved            | RÂ² = 0.053 | RMSE = 14.039\n",
      "confidentâ€“insecure                  | RÂ² = 0.056 | RMSE = 17.969\n",
      "selfishâ€“altruistic                  | RÂ² = 0.012 | RMSE = 13.006\n",
      "angelicâ€“demonic                     | RÂ² = 0.117 | RMSE = 18.286\n",
      "cruelâ€“kind                          | RÂ² = 0.065 | RMSE = 15.148\n",
      "directâ€“roundabout                   | RÂ² = 0.042 | RMSE = 17.448\n",
      "biasedâ€“impartial                    | RÂ² = 0.041 | RMSE = 17.930\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.024 | RMSE = 19.015\n",
      "judgementalâ€“accepting               | RÂ² = 0.040 | RMSE = 17.282\n",
      "complimentaryâ€“insulting             | RÂ² = 0.050 | RMSE = 14.961\n",
      "wholesomeâ€“salacious                 | RÂ² = 0.021 | RMSE = 18.427\n",
      "zanyâ€“regular                        | RÂ² = 0.090 | RMSE = 18.464\n",
      "racistâ€“egalitarian                  | RÂ² = 0.028 | RMSE = 15.733\n",
      "transparentâ€“machiavellian           | RÂ² = 0.090 | RMSE = 18.985\n",
      "innocentâ€“jaded                      | RÂ² = 0.051 | RMSE = 17.322\n",
      "flawedâ€“perfect                      | RÂ² = 0.086 | RMSE = 19.535\n",
      "resentfulâ€“euphoric                  | RÂ² = 0.027 | RMSE = 19.502\n",
      "buffoonâ€“charmer                     | RÂ² = 0.035 | RMSE = 14.031\n",
      "fakeâ€“real                           | RÂ² = 0.029 | RMSE = 20.840\n",
      "cattyâ€“supportive                    | RÂ² = 0.016 | RMSE = 19.794\n",
      "eagerâ€“reluctant                     | RÂ² = 0.021 | RMSE = 18.434\n",
      "forwardâ€“repressed                   | RÂ² = 0.107 | RMSE = 19.744\n",
      "maverickâ€“conformist                 | RÂ² = 0.012 | RMSE = 19.696\n",
      "social chameleonâ€“strong identity    | RÂ² = 0.053 | RMSE = 19.826\n",
      "sincereâ€“irreverent                  | RÂ² = 0.030 | RMSE = 18.380\n",
      "hopefulâ€“fearful                     | RÂ² = 0.008 | RMSE = 18.197\n",
      "likes changeâ€“resists change         | RÂ² = 0.024 | RMSE = 18.962\n",
      "old-fashionedâ€“progressive           | RÂ² = 0.037 | RMSE = 19.351\n"
     ]
    }
   ],
   "source": [
    "run_moral_trait_prediction(\n",
    "    fasttext_embedding_dictionary=fasttext_embedding_dictionary,\n",
    "    structured_data_full=structured_data_full,\n",
    "    trait_index_to_name=trait_index_to_name,\n",
    "    moral_trait_indices=moral_trait_indices,\n",
    "    reduction_method=\"pca\",  # or \"ae\"\n",
    "    model_type=\"ridge\",      # or \"mlp\"\n",
    "    latent_dim=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training PCA reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 35)\n",
      "Training MLP model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = -0.107 | RMSE = 22.559\n",
      "ferociousâ€“pacifist                  | RÂ² = -0.087 | RMSE = 22.218\n",
      "forgivingâ€“vengeful                  | RÂ² = -0.217 | RMSE = 19.183\n",
      "loyalâ€“traitorous                    | RÂ² = -0.159 | RMSE = 20.442\n",
      "rudeâ€“respectful                     | RÂ² = 0.005 | RMSE = 15.694\n",
      "arrogantâ€“humble                     | RÂ² = -0.004 | RMSE = 22.233\n",
      "heroicâ€“villainous                   | RÂ² = 0.034 | RMSE = 17.639\n",
      "mischievousâ€“well-behaved            | RÂ² = -0.956 | RMSE = 20.173\n",
      "confidentâ€“insecure                  | RÂ² = -0.210 | RMSE = 20.347\n",
      "selfishâ€“altruistic                  | RÂ² = -1.155 | RMSE = 19.213\n",
      "angelicâ€“demonic                     | RÂ² = -0.369 | RMSE = 22.772\n",
      "cruelâ€“kind                          | RÂ² = -0.284 | RMSE = 17.750\n",
      "directâ€“roundabout                   | RÂ² = -0.450 | RMSE = 21.464\n",
      "biasedâ€“impartial                    | RÂ² = -0.631 | RMSE = 23.385\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.016 | RMSE = 19.094\n",
      "judgementalâ€“accepting               | RÂ² = -0.326 | RMSE = 20.317\n",
      "complimentaryâ€“insulting             | RÂ² = -0.316 | RMSE = 17.605\n",
      "wholesomeâ€“salacious                 | RÂ² = -0.374 | RMSE = 21.827\n",
      "zanyâ€“regular                        | RÂ² = -0.142 | RMSE = 20.682\n",
      "racistâ€“egalitarian                  | RÂ² = -0.420 | RMSE = 19.016\n",
      "transparentâ€“machiavellian           | RÂ² = -0.266 | RMSE = 22.395\n",
      "innocentâ€“jaded                      | RÂ² = -0.238 | RMSE = 19.782\n",
      "flawedâ€“perfect                      | RÂ² = -0.099 | RMSE = 21.422\n",
      "resentfulâ€“euphoric                  | RÂ² = -0.254 | RMSE = 22.147\n",
      "buffoonâ€“charmer                     | RÂ² = -1.116 | RMSE = 20.780\n",
      "fakeâ€“real                           | RÂ² = -0.216 | RMSE = 23.320\n",
      "cattyâ€“supportive                    | RÂ² = -0.121 | RMSE = 21.128\n",
      "eagerâ€“reluctant                     | RÂ² = -0.098 | RMSE = 19.529\n",
      "forwardâ€“repressed                   | RÂ² = -0.009 | RMSE = 20.988\n",
      "maverickâ€“conformist                 | RÂ² = -0.354 | RMSE = 23.059\n",
      "social chameleonâ€“strong identity    | RÂ² = -0.237 | RMSE = 22.655\n",
      "sincereâ€“irreverent                  | RÂ² = -0.306 | RMSE = 21.326\n",
      "hopefulâ€“fearful                     | RÂ² = -0.390 | RMSE = 21.544\n",
      "likes changeâ€“resists change         | RÂ² = -0.273 | RMSE = 21.652\n",
      "old-fashionedâ€“progressive           | RÂ² = -0.887 | RMSE = 27.090\n"
     ]
    }
   ],
   "source": [
    "run_moral_trait_prediction(\n",
    "    fasttext_embedding_dictionary=fasttext_embedding_dictionary,\n",
    "    structured_data_full=structured_data_full,\n",
    "    trait_index_to_name=trait_index_to_name,\n",
    "    moral_trait_indices=moral_trait_indices,\n",
    "    reduction_method=\"pca\",  # or \"ae\"\n",
    "    model_type=\"mlp\",      # or \"mlp\"\n",
    "    latent_dim=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training AE reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 35)\n",
      "Training RIDGE model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = 0.008 | RMSE = 21.349\n",
      "ferociousâ€“pacifist                  | RÂ² = 0.017 | RMSE = 21.134\n",
      "forgivingâ€“vengeful                  | RÂ² = 0.037 | RMSE = 17.067\n",
      "loyalâ€“traitorous                    | RÂ² = 0.004 | RMSE = 18.952\n",
      "rudeâ€“respectful                     | RÂ² = 0.029 | RMSE = 15.501\n",
      "arrogantâ€“humble                     | RÂ² = 0.011 | RMSE = 22.059\n",
      "heroicâ€“villainous                   | RÂ² = 0.017 | RMSE = 17.798\n",
      "mischievousâ€“well-behaved            | RÂ² = 0.033 | RMSE = 14.186\n",
      "confidentâ€“insecure                  | RÂ² = 0.039 | RMSE = 18.128\n",
      "selfishâ€“altruistic                  | RÂ² = 0.007 | RMSE = 13.045\n",
      "angelicâ€“demonic                     | RÂ² = 0.087 | RMSE = 18.593\n",
      "cruelâ€“kind                          | RÂ² = 0.047 | RMSE = 15.287\n",
      "directâ€“roundabout                   | RÂ² = 0.026 | RMSE = 17.589\n",
      "biasedâ€“impartial                    | RÂ² = 0.025 | RMSE = 18.082\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.008 | RMSE = 19.174\n",
      "judgementalâ€“accepting               | RÂ² = 0.023 | RMSE = 17.436\n",
      "complimentaryâ€“insulting             | RÂ² = 0.029 | RMSE = 15.121\n",
      "wholesomeâ€“salacious                 | RÂ² = 0.012 | RMSE = 18.512\n",
      "zanyâ€“regular                        | RÂ² = 0.068 | RMSE = 18.684\n",
      "racistâ€“egalitarian                  | RÂ² = 0.018 | RMSE = 15.808\n",
      "transparentâ€“machiavellian           | RÂ² = 0.056 | RMSE = 19.337\n",
      "innocentâ€“jaded                      | RÂ² = 0.038 | RMSE = 17.442\n",
      "flawedâ€“perfect                      | RÂ² = 0.060 | RMSE = 19.809\n",
      "resentfulâ€“euphoric                  | RÂ² = 0.017 | RMSE = 19.607\n",
      "buffoonâ€“charmer                     | RÂ² = 0.019 | RMSE = 14.148\n",
      "fakeâ€“real                           | RÂ² = 0.018 | RMSE = 20.961\n",
      "cattyâ€“supportive                    | RÂ² = 0.007 | RMSE = 19.886\n",
      "eagerâ€“reluctant                     | RÂ² = 0.010 | RMSE = 18.546\n",
      "forwardâ€“repressed                   | RÂ² = 0.075 | RMSE = 20.098\n",
      "maverickâ€“conformist                 | RÂ² = 0.002 | RMSE = 19.793\n",
      "social chameleonâ€“strong identity    | RÂ² = 0.035 | RMSE = 20.010\n",
      "sincereâ€“irreverent                  | RÂ² = 0.016 | RMSE = 18.508\n",
      "hopefulâ€“fearful                     | RÂ² = 0.002 | RMSE = 18.255\n",
      "likes changeâ€“resists change         | RÂ² = 0.010 | RMSE = 19.095\n",
      "old-fashionedâ€“progressive           | RÂ² = 0.014 | RMSE = 19.585\n"
     ]
    }
   ],
   "source": [
    "run_moral_trait_prediction(\n",
    "    fasttext_embedding_dictionary=fasttext_embedding_dictionary,\n",
    "    structured_data_full=structured_data_full,\n",
    "    trait_index_to_name=trait_index_to_name,\n",
    "    moral_trait_indices=moral_trait_indices,\n",
    "    reduction_method=\"ae\",  # or \"ae\"\n",
    "    model_type=\"ridge\",      # or \"mlp\"\n",
    "    latent_dim=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training AE reducer...\n",
      "Building feature matrix...\n",
      "Final shape: X = (201, 20), y = (201, 35)\n",
      "Training MLP model...\n",
      "\n",
      " Moral Trait Prediction Results:\n",
      "cunningâ€“honorable                   | RÂ² = -0.178 | RMSE = 23.265\n",
      "ferociousâ€“pacifist                  | RÂ² = -0.046 | RMSE = 21.795\n",
      "forgivingâ€“vengeful                  | RÂ² = -0.263 | RMSE = 19.544\n",
      "loyalâ€“traitorous                    | RÂ² = -0.225 | RMSE = 21.019\n",
      "rudeâ€“respectful                     | RÂ² = -0.037 | RMSE = 16.019\n",
      "arrogantâ€“humble                     | RÂ² = 0.003 | RMSE = 22.147\n",
      "heroicâ€“villainous                   | RÂ² = 0.032 | RMSE = 17.661\n",
      "mischievousâ€“well-behaved            | RÂ² = -0.636 | RMSE = 18.450\n",
      "confidentâ€“insecure                  | RÂ² = -0.129 | RMSE = 19.651\n",
      "selfishâ€“altruistic                  | RÂ² = -0.761 | RMSE = 17.366\n",
      "angelicâ€“demonic                     | RÂ² = 0.020 | RMSE = 19.266\n",
      "cruelâ€“kind                          | RÂ² = -0.206 | RMSE = 17.201\n",
      "directâ€“roundabout                   | RÂ² = -0.388 | RMSE = 20.998\n",
      "biasedâ€“impartial                    | RÂ² = -0.233 | RMSE = 20.328\n",
      "sarcasticâ€“genuine                   | RÂ² = 0.027 | RMSE = 18.989\n",
      "judgementalâ€“accepting               | RÂ² = -0.183 | RMSE = 19.191\n",
      "complimentaryâ€“insulting             | RÂ² = -0.066 | RMSE = 15.842\n",
      "wholesomeâ€“salacious                 | RÂ² = -0.384 | RMSE = 21.908\n",
      "zanyâ€“regular                        | RÂ² = -0.090 | RMSE = 20.209\n",
      "racistâ€“egalitarian                  | RÂ² = -0.332 | RMSE = 18.415\n",
      "transparentâ€“machiavellian           | RÂ² = -0.079 | RMSE = 20.677\n",
      "innocentâ€“jaded                      | RÂ² = -0.177 | RMSE = 19.292\n",
      "flawedâ€“perfect                      | RÂ² = -0.068 | RMSE = 21.115\n",
      "resentfulâ€“euphoric                  | RÂ² = -0.088 | RMSE = 20.628\n",
      "buffoonâ€“charmer                     | RÂ² = -0.493 | RMSE = 17.453\n",
      "fakeâ€“real                           | RÂ² = -0.195 | RMSE = 23.119\n",
      "cattyâ€“supportive                    | RÂ² = -0.024 | RMSE = 20.202\n",
      "eagerâ€“reluctant                     | RÂ² = -0.138 | RMSE = 19.879\n",
      "forwardâ€“repressed                   | RÂ² = 0.078 | RMSE = 20.056\n",
      "maverickâ€“conformist                 | RÂ² = -0.269 | RMSE = 22.322\n",
      "social chameleonâ€“strong identity    | RÂ² = 0.013 | RMSE = 20.235\n",
      "sincereâ€“irreverent                  | RÂ² = -0.153 | RMSE = 20.039\n",
      "hopefulâ€“fearful                     | RÂ² = -0.387 | RMSE = 21.520\n",
      "likes changeâ€“resists change         | RÂ² = -0.158 | RMSE = 20.650\n",
      "old-fashionedâ€“progressive           | RÂ² = -0.381 | RMSE = 23.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run_moral_trait_prediction(\n",
    "    fasttext_embedding_dictionary=fasttext_embedding_dictionary,\n",
    "    structured_data_full=structured_data_full,\n",
    "    trait_index_to_name=trait_index_to_name,\n",
    "    moral_trait_indices=moral_trait_indices,\n",
    "    reduction_method=\"ae\",  # or \"ae\"\n",
    "    model_type=\"mlp\",      # or \"mlp\"\n",
    "    latent_dim=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the model predict for iconic characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_moral_trait_prediction_with_model(\n",
    "    fasttext_embedding_dictionary,\n",
    "    structured_data_full,\n",
    "    trait_index_to_name,\n",
    "    moral_trait_indices,\n",
    "    sentence_types=[\"moral\", \"non_moral\", \"action\", \"adj\"],\n",
    "    reduction_method=\"pca\",\n",
    "    model_type=\"ridge\",\n",
    "    latent_dim=20,\n",
    "    ae_epochs=100\n",
    "):\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    print(\"ðŸ“¦ Collecting all sentence embeddings...\")\n",
    "    all_embeddings = []\n",
    "    for movie in fasttext_embedding_dictionary:\n",
    "        for char in fasttext_embedding_dictionary[movie]:\n",
    "            for t in sentence_types:\n",
    "                all_embeddings.extend(fasttext_embedding_dictionary[movie][char].get(t, []))\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "    print(f\"ðŸ”§ Training {reduction_method.upper()} reducer...\")\n",
    "    if reduction_method == \"pca\":\n",
    "        reducer = PCA(n_components=latent_dim)\n",
    "        reducer.fit(all_embeddings)\n",
    "        reduce_fn = lambda X: reducer.transform(X)\n",
    "    elif reduction_method == \"ae\":\n",
    "        class AE(nn.Module):\n",
    "            def __init__(self, input_dim=384, latent_dim=latent_dim):\n",
    "                super().__init__()\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(input_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, latent_dim)\n",
    "                )\n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, input_dim)\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                z = self.encoder(x)\n",
    "                return z, self.decoder(z)\n",
    "\n",
    "        ae = AE()\n",
    "        optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        X_tensor = torch.tensor(all_embeddings, dtype=torch.float32)\n",
    "        for epoch in range(ae_epochs):\n",
    "            ae.train()\n",
    "            z, recon = ae(X_tensor)\n",
    "            loss = loss_fn(recon, X_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        ae.eval()\n",
    "        reduce_fn = lambda X: ae.encoder(torch.tensor(X, dtype=torch.float32)).detach().cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reducer.\")\n",
    "\n",
    "    print(\"Building feature matrix...\")\n",
    "    X, y, movie_char_pairs = [], [], []\n",
    "    for movie, chars in fasttext_embedding_dictionary.items():\n",
    "        for char, data in chars.items():\n",
    "            all_sentences = []\n",
    "            for t in sentence_types:\n",
    "                all_sentences.extend(data.get(t, []))\n",
    "            if not all_sentences:\n",
    "                continue\n",
    "            reduced = reduce_fn(np.vstack(all_sentences))\n",
    "            avg_vector = reduced.mean(axis=0)\n",
    "            X.append(avg_vector)\n",
    "            movie_char_pairs.append((movie, char))\n",
    "            if \"rating\" in structured_data_full[movie][char] and len(structured_data_full[movie][char][\"rating\"]) >= max(moral_trait_indices) + 1:\n",
    "                y.append([structured_data_full[movie][char][\"rating\"][i] for i in moral_trait_indices])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Training {model_type.upper()} model...\")\n",
    "    if model_type == \"ridge\":\n",
    "        model = Ridge()\n",
    "    elif model_type == \"mlp\":\n",
    "        mlp_model = MLPRegressor(\n",
    "            hidden_layer_sizes=(32,),\n",
    "            max_iter=2000,\n",
    "            early_stopping=True,\n",
    "            learning_rate='adaptive',\n",
    "            random_state=42,\n",
    "            alpha=0.01\n",
    "        )\n",
    "        model = make_pipeline(StandardScaler(), mlp_model)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type.\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model, reduce_fn, movie_char_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-defining the function since kernel was reset\n",
    "def extract_selected_character_predictions(\n",
    "    fasttext_embedding_dictionary,\n",
    "    structured_data_full,\n",
    "    trait_index_to_name,\n",
    "    moral_trait_indices,\n",
    "    model,\n",
    "    reduce_fn,\n",
    "    sentence_types=[\"moral\", \"non_moral\", \"action\", \"adj\"],\n",
    "    selected_characters=None\n",
    "):\n",
    "    result_rows = []\n",
    "\n",
    "    for movie, chars in fasttext_embedding_dictionary.items():\n",
    "        for char, data in chars.items():\n",
    "            if selected_characters and (movie not in selected_characters or char not in selected_characters[movie]):\n",
    "                continue\n",
    "\n",
    "            all_sentences = []\n",
    "            for t in sentence_types:\n",
    "                all_sentences.extend(data.get(t, []))\n",
    "\n",
    "            if not all_sentences:\n",
    "                continue\n",
    "\n",
    "            reduced = reduce_fn(np.vstack(all_sentences))\n",
    "            avg_vector = reduced.mean(axis=0)\n",
    "\n",
    "            if \"rating\" not in structured_data_full[movie][char]:\n",
    "                continue\n",
    "\n",
    "            char_rating = structured_data_full[movie][char][\"rating\"]\n",
    "            if len(char_rating) < max(moral_trait_indices) + 1:\n",
    "                continue\n",
    "\n",
    "            true_vector = [char_rating[i] for i in moral_trait_indices]\n",
    "            pred_vector = model.predict([avg_vector])[0]\n",
    "\n",
    "            for idx, trait_idx in enumerate(moral_trait_indices):\n",
    "                trait_name = trait_index_to_name.get(trait_idx, f\"Trait {trait_idx}\")\n",
    "                result_rows.append({\n",
    "                    \"Character\": f\"{movie}_{char}\",\n",
    "                    \"Trait\": trait_name,\n",
    "                    \"Ground Truth\": true_vector[idx],\n",
    "                    \"Predicted Rating\": pred_vector[idx]\n",
    "                })\n",
    "\n",
    "    selected_results_df = pd.DataFrame(result_rows)\n",
    "    return selected_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_characters = {\n",
    "    \"The Dark Knight\": [\"THE JOKER\"],\n",
    "    \"Toy Story\": [\"WOODY\"],\n",
    "    \"The Shawshank Redemption\": [\"RED\"],\n",
    "    \"Star Wars: Episode IV - A New Hope\": [\"LUKE\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Collecting all sentence embeddings...\n",
      "ðŸ”§ Training PCA reducer...\n",
      "Building feature matrix...\n",
      "Training RIDGE model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, reduce_fn, movie_char_pairs = run_moral_trait_prediction_with_model(\n",
    "    fasttext_embedding_dictionary=fasttext_embedding_dictionary,\n",
    "    structured_data_full=structured_data_full,\n",
    "    trait_index_to_name=trait_index_to_name,\n",
    "    moral_trait_indices=moral_trait_indices,\n",
    "    reduction_method=\"pca\",\n",
    "    model_type=\"ridge\"\n",
    ")\n",
    "\n",
    "df_preds = extract_selected_character_predictions(\n",
    "    model=model,\n",
    "    reduce_fn=reduce_fn,\n",
    "    selected_characters=selected_characters,\n",
    "    fasttext_embedding_dictionary=fasttext_embedding_dictionary,\n",
    "    structured_data_full=structured_data_full,\n",
    "    trait_index_to_name=trait_index_to_name,\n",
    "    moral_trait_indices=moral_trait_indices,\n",
    "    sentence_types=[\"moral\", \"non_moral\", \"action\", \"adj\"]\n",
    ")\n",
    "\n",
    "joker_result_df = df_preds[df_preds[\"Character\"] == \"The Dark Knight_THE JOKER\"]\n",
    "woody_result_df = df_preds[df_preds[\"Character\"] == \"Toy Story_WOODY\"]\n",
    "red_result_df = df_preds[df_preds[\"Character\"] == \"The Shawshank Redemption_RED\"]\n",
    "luke_result_df = df_preds[df_preds[\"Character\"] == \"Star Wars: Episode IV - A New Hope_LUKE\"]\n",
    "\n",
    "joker_result_df = joker_result_df.drop(columns=[\"Character\"])\n",
    "woody_result_df = woody_result_df.drop(columns=[\"Character\"])\n",
    "red_result_df = red_result_df.drop(columns=[\"Character\"])\n",
    "luke_result_df = luke_result_df.drop(columns=[\"Character\"])\n",
    "\n",
    "\n",
    "joker_result_df.to_excel(\"..//results//baseline_2_joker_result.xlsx\", index=False)\n",
    "woody_result_df.to_excel(\"..//results//baseline_2_woody_result.xlsx\", index=False)\n",
    "red_result_df.to_excel(\"..//results//baseline_2_red_result.xlsx\", index=False)\n",
    "luke_result_df.to_excel(\"..//results//baseline_2_luke_result.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
